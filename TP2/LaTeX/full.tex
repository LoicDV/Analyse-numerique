\documentclass[a4paper, 10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{pdfpages}
\usepackage{algorithmic}
\usepackage[french,linesnumbered,algoruled]{algorithm2e}
\geometry{hmargin=2cm,vmargin=1.5cm}
%\renewcommand{\baselinestretch}{0.96}
\newcommand{\IR}{\mathbb{R}}
\newcommand{\IN}{\mathbb{N}}
\newcommand{\inte}{\operatorname{int}}
\newcommand{\Image}{\operatorname{Im}}
\newcommand{\EFFECT}{\textbf{Effet : }}
\renewcommand{\algorithmicrequire}{\textbf{Entrée :}}
\renewcommand{\algorithmicensure}{\textbf{Sortie :}}
\renewcommand{\algorithmicend}{\textbf{fin}}
\renewcommand{\algorithmicif}{\textbf{si}}
\renewcommand{\algorithmicthen}{\textbf{alors}}
\renewcommand{\algorithmicelse}{\textbf{sinon}}
\renewcommand{\algorithmicelsif}{\algorithmicelse\ \algorithmicif}
\renewcommand{\algorithmicendif}{\algorithmicend\ \algorithmicif}
\renewcommand{\algorithmicfor}{\textbf{pour}}
\renewcommand{\algorithmicforall}{\textbf{pour tout}}
\renewcommand{\algorithmicdo}{\textbf{faire}}
\renewcommand{\algorithmicendfor}{\algorithmicend\ \algorithmicfor}
\renewcommand{\algorithmicwhile}{\textbf{tant que}}
\renewcommand{\algorithmicendwhile}{\algorithmicend\ \algorithmicwhile}
\renewcommand{\algorithmicloop}{\textbf{boucle}}
\renewcommand{\algorithmicendloop}{\algorithmicend\ \algorithmicloop}
\renewcommand{\algorithmicrepeat}{\textbf{répéter}}
\renewcommand{\algorithmicuntil}{\textbf{jusqu'à}}
\renewcommand{\algorithmicprint}{\textbf{imprimer}}
\renewcommand{\algorithmicreturn}{\textbf{retourner}}
\renewcommand{\algorithmictrue}{\textbf{vrai}}
\renewcommand{\algorithmicfalse}{\textbf{faux}}
\makeatletter
\renewcommand{\algocf@captiontext}[2]{\AlCapNameSty{\centerline{\AlCapNameFnt{}#2}}}
\makeatother

\begin{document}

\begin{titlepage}
\begin{center}

{\Large Université de Mons}\\[1ex]
{\Large Faculté des Sciences}\\[1ex]
{\Large Département des mathématiques}\\[2.5cm]

\newcommand{\HRule}{\rule{\linewidth}{0.3mm}}
% Title
\HRule \\[0.3cm]
{ \LARGE \bfseries Rapport du TP2 \\[0.3cm]}
{ \LARGE \bfseries Analyse Numérique \\[0.1cm]}
\HRule \\[1.5cm]

% Author and supervisor
\begin{minipage}[t]{0.45\textwidth}
\begin{flushleft} \large
\emph{Professeur:}\\
Christophe \textsc{Troestler} \\
Quentin \textsc{Lambotte}
\end{flushleft}
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
\begin{flushright} \large
\emph{Auteurs:} \\
Loïc \textsc{Dupont} \\
Paolo \textsc{Marcelis} \\
Maximilien \textsc{Vanhaverbeke}
\end{flushright}
\end{minipage}\\[2ex]

\vfill

% Bottom of the page
\begin{center}
\begin{tabular}[t]{c c c}
\includegraphics[height=1.5cm]{logoumons.jpg} &
\hspace{0.3cm} &
\includegraphics[height=1.5cm]{logofs.jpg}
\end{tabular}
\end{center}~\\
 
{\large Année académique 2020-2021}

\end{center}
\end{titlepage}

\renewcommand{\contentsname}{Table des matières}
\tableofcontents

\newpage

\noindent
Soit \( M, N \in \IN \). Soit \( A \in \IR^{M \times N} \) avec rang\( A = N \leq M \). Soit \( b \in \mathbb{R}^n \). \\
Intéressons-nous à la résolution de l'équation \( Ax = b \). \\
Les exercices suivants consistent à regarder la meilleur solution au sens des moindres carrés càd la valeur de x qui minimise la fonction :
\begin{equation}
    \label{eq:1}
    x \mapsto |Ax - b|_2
\end{equation}
Pour les questions suivantes, nous allons utiliser les notations ci-dessous :
$$
A =
\begin{pmatrix}
a_{1, 1} & \cdots & a_{1, N} \\
\vdots   &        & \vdots \\
a_{M, 1} & \cdots & a_{M, N}
\end{pmatrix}
= (a_{i, j})_{
\tiny \begin{matrix}
1 \leq i \leq M \\
1 \leq j \leq N
\end{matrix}}
, \quad b =
\begin{pmatrix}
b_1 \\
\vdots \\
b_M
\end{pmatrix}
= (b_i)_{1 \leq i \leq M}, \quad x =
\begin{pmatrix} x_1 \\
\vdots \\
x_N
\end{pmatrix}
= (x_j)_{1 \leq j \leq N}
$$

\section{Exercice 1}

Soit \( x \in \IR^N \). On doit montrer que \( x \) réalise le minimum de \( x \mapsto |Ax - b|_2 \) \eqref{eq:1} si et seulement si \( x \) réalise le minimum de
\begin{equation}
    \label{eq:2}
    x \mapsto |Ax - b|_2^2
\end{equation}
Commençons par montrer que si \( x \) réalise le minimum de \eqref{eq:1}, alors \( x \) réalise le minimum de \eqref{eq:2}. \\
Supposons que \( x \) réalise le minimum de \eqref{eq:1} càd
$$
\forall y \in \IR^N,~ |Ax - b|_2 \leq |Ay - b|_2
$$
Comme toute norme est définie positive, on sait que pour tout \( z \in \IR^M,~ |z|_2 \in [0, +\infty[ \). \\
Alors, comme chaque membre de l'inégalité ci-dessus est positif, par croissance de la fonction \( x \mapsto x^2 \) sur \( [0, +\infty[ \), on a
$$
\forall y \in \IR^N,~ |Ax - b|_2^2 \leq |Ay - b|_2^2
$$
càd \( x \) réalise le minimum de \eqref{eq:2}. \\
Il reste à montrer que si \( x \) réalise le minimum de \eqref{eq:2}, alors \( x \) réalise le minimum de \eqref{eq:1}. \\
Supposons que \( x \) réalise le minimum de \eqref{eq:2} càd
$$
\forall y \in \IR^N,~ |Ax - b|_2^2 \leq |Ay - b|_2^2
$$
Comme tout carré d'un nombre réel est positif, par croissance de la fonction \( x \mapsto \sqrt{x} \) sur \( [0, +\infty[ \), on a
$$
\forall y \in \IR^N,~ \sqrt{|Ax - b|_2^2} \leq \sqrt{|Ay - b|_2^2}
$$
Comme les normes sont définies positives, \( \sqrt{|Ax - b|_2^2} = |Ax - b|_2 \) et \( \forall y \in \IR^N,~ \sqrt{|Ay - b|_2^2} = |Ay - b|_2 \). \\
Par conséquent, on a bien
$$
\forall y \in \IR^N,~ |Ax - b|_2 \leq |Ay - b|_2
$$
càd \( x \) réalise le minimum de \eqref{eq:1}. \\
On a donc bien montré que \( x \) réalise le minimum de \eqref{eq:1}, alors \( x \) réalise le minimum de \eqref{eq:2}.

\newpage

\section{Exercice 2}

Soit \( x \in \IR^N \). On doit montrer que \( x \) réalise le minimum de \( x \mapsto |Ax - b|_2 \) \eqref{eq:1} si et seulement si \( x \) vérifie l'équation
\begin{equation}
    \label{eq:3}
    A^T A x = A^T b
\end{equation}
Commençons par montrer que si \( x \) réalise le minimum de \eqref{eq:1}, alors \( x \) vérifie l'équation \eqref{eq:3}. \\
Supposons que \( x \) réalise le minimum de \eqref{eq:1} càd
$$
\forall y \in \IR^N,~ |Ax - b|_2 \leq |Ay - b|_2
$$
Par l'exercice 1, on sait que c'est équivalent à dire que
$$
\forall y \in \IR^N,~ |Ax - b|_2^2 \leq |Ay - b|_2^2
$$
Commençons par calculer explicitement \( f : x \mapsto |Ax - b|_2^2 \). On a
\begin{align*}
    |Ax - b|_2^2
    & = \left|
        \begin{pmatrix}
        a_{1, 1} & \cdots & a_{1, N} \\
        \vdots   &        & \vdots \\
        a_{M, 1} & \cdots & a_{M, N}
        \end{pmatrix}
        \begin{pmatrix}
        x_1 \\
        \vdots \\
        x_N
        \end{pmatrix}
        -
        \begin{pmatrix}
        b_1 \\
        \vdots \\
        b_M
        \end{pmatrix}
        \right|_2^2 \\
    & = \left|
        \begin{pmatrix}
        \displaystyle \left( \sum_{j = 1}^N a_{1, j} x_j \right) - b_1 \\
        \vdots \\
        \displaystyle \left( \sum_{j = 1}^N a_{M, j} x_j \right) - b_M
        \end{pmatrix}
        \right|_2^2 \\
    & = \left( \sqrt{\sum_{i = 1}^M \left( \left( \sum_{j = 1}^N a_{i, j} x_j \right) - b_i \right)^2} \right)^2 \\
    & = \sum_{i = 1}^M \left( \left( \sum_{j = 1}^N a_{i, j} x_j \right) - b_i \right)^2
\end{align*}
On sait donc que \( x \) réalise le minimum de
$$
f : \IR^n \to \IR : x = (x_1, \cdots, x_n) \mapsto \sum_{i = 1}^M \left( \left( \sum_{j = 1}^N a_{i, j} x_j \right) - b_i \right)^2
$$
En particulier, il réalise également le minimum de toute restriction de \( f \) à un ensemble contenant \( x \). \\
On sait donc que pour tout \( k \in \IN^{\leq n} \), \( x \) réalise le minimum de
$$
f_k : \IR \to \IR : x_k \mapsto \sum_{i = 1}^M \left( \left( \sum_{j = 1}^N a_{i, j} x_j \right) - b_i \right)^2
$$
Ces fonctions sont dérivables sur \( \IR \) et pour tout \( k \in \IN^{\leq N} \),
\begin{align*}
    \partial_k \sum_{i = 1}^M \left( \left( \sum_{j = 1}^N a_{i, j} x_j \right) - b_i \right)^2
    & = \sum_{i = 1}^M \left( \left( 2 \sum_{j = 1}^N a_{i, j} x_j \right) - 2 b_i \right) \partial_k \left( \left( \sum_{j = 1}^N a_{i, j} x_j \right) - b_i \right) \\
    & = \sum_{i = 1}^M \left( \left( 2 \sum_{j = 1}^N a_{i, j} x_j \right) - 2 b_i \right) a_{i, k}
\end{align*}

\newpage

\noindent
On sait que \( x \in \inte \IR = \IR \) et qu'il s'agit d'un minimum global de \( f_k \) et donc en particulier un minimum local. La propriété VI.16 du syllabus d'analyse 1 nous dit alors que la dérivée de \( f_k \) s'annule en \( x \). On sait donc que
\begin{align*}
    \forall k \in \IN^{\leq N},~ \partial f_k = 0
    & \quad \text{ càd } \quad \forall k \in \IN^{\leq N},~ \sum_{i = 1}^M \left( \left( 2 \sum_{j = 1}^N a_{i, j} x_j \right) - 2 b_i \right) a_{i, k} = 0 \\
    & \quad \text{ càd } \quad
        \begin{pmatrix}
        \displaystyle \sum_{i = 1}^M \left( \left( 2 \sum_{j = 1}^N a_{i, j} x_j \right) - 2 b_i \right) a_{i, 1} \\
        \vdots \\
        \displaystyle \sum_{i = 1}^M \left( \left( 2 \sum_{j = 1}^N a_{i, j} x_j \right) - 2 b_i \right) a_{i, N}
        \end{pmatrix}
        =
        \begin{pmatrix}
        0 \\
        \vdots \\
        0
        \end{pmatrix} \\
    & \quad \text{ càd } \quad
        2 \begin{pmatrix}
        \displaystyle \sum_{i = 1}^M \left( a_{i, 1} \sum_{j = 1}^N \left( a_{i, j} x_j \right) \right) \\
        \vdots \\
        \displaystyle \sum_{i = 1}^M \left( a_{i, N} \sum_{j = 1}^N \left( a_{i, j} x_j \right) \right)
        \end{pmatrix}
        -
        2 \begin{pmatrix}
        \displaystyle \sum_{i = 1}^M \left( a_{i, 1} b_i \right) \\
        \vdots \\
        \displaystyle \sum_{i = 1}^M \left( a_{i, N} b_i \right)
        \end{pmatrix}
        =
        \begin{pmatrix}
        0 \\
        \vdots \\
        0
        \end{pmatrix} \\
    & \quad \text{ càd } \quad 2 A^T A x - 2 A^T b = 0 \\
    & \quad \text{ càd } \quad A^T A x = A^T b
\end{align*}
On a donc bien montré que si \( x \) réalise le minimum de \eqref{eq:1}, alors \( x \) vérifie l'équation \eqref{eq:3}. \\
Il reste à monter que si \( x \) vérifie l'équation \eqref{eq:3}, alors \( x \) réalise le minimum de \eqref{eq:1}. \\
Supposons que \( x \) vérifie l'équation \eqref{eq:3} càd \( A^T A x = A^T b \). \\
On sait que \( \forall y \in \IR^N,~ |Ay - b|_2^2 = (Ay - b ~|~ Ay - b) = (Ay ~|~ Ay) - 2(Ay ~|~ b) + (b ~|~ b) \) par bilinéarité du produit scalaire. Par conséquent
\begin{align*}
    x \text{ réalise le minimum de } \eqref{eq:1}
    & \Longleftrightarrow \forall y \in \IR^n,~ |Ax - b|_2^2 \leq |Ay - b|_2^2 \\
    & \Longleftrightarrow (Ax - b ~|~ Ax - b) \leq (Ay - b ~|~ Ay - b) \\
    & \Longleftrightarrow (Ax ~|~ Ax) - 2(Ax ~|~ b) \leq (Ay ~|~ Ay) - 2(Ay ~|~ b)
\end{align*}
Par défintion du produit scalaire, on a
$$
\forall u, v \in \IR^n,~ (u ~|~ v) = \sum_{i = 1}^n (u_i v_i)
$$
On remarque donc que \( \forall u, v \in \IR^M,~ (u ~|~ v) \) est égal à l'unique terme de la matrice \( u^T v \). \\
De plus, par hypothèse on sait que \( A^TAx = A^Tb \) et donc \( (Ax)^T Ax = x^T A^T A x = x^T A^Tb = (Ax)^T b \). \\
Par conséquent \( (Ax ~|~ Ax) = (Ax ~|~ b) \) et donc
\begin{align*}
    x \text{ réalise le minimum de } \eqref{eq:1}
    & \Longleftrightarrow (Ax ~|~ b) - 2(Ax ~|~ b) \leq (Ay ~|~ Ay) - 2(Ay ~|~ b) \\
    & \Longleftrightarrow 0 \leq (Ay ~|~ Ay) - 2(Ay ~|~ b) + (Ax ~|~ b)
\end{align*}
Montrons que cette inégalité est toujours vraie. Pour celà, nous allons la minorer par une valeur positive. \\
$$
\textbf{Nous n'avons pas réussi à montrer l'inégalité.}
$$
Puisque l'inégalité est toujours vraie, on a bien montré que \( x \) réalise le minimum de \eqref{eq:1}.

\newpage

\section{Exercice 3}

On doit montrer que sous les hypothèses sous lesquelles nous travaillons, énoncées au sommet de la page 2, l'équation \eqref{eq:3} possède toujours une unique solution. \\
Commençons par montrer l'existance d'une telle solution. On sait par les exercices 1 et 2 que si \( x \in \IR^N \) est un point qui réalise le minimum de \eqref{eq:1}, alors \( x \) vérifie l'équation \eqref{eq:3}. Montrons donc que \eqref{eq:1} admet un minimum. \\
Tout d'abord, on sait par le cours d'algèbre linéaire que \( x \mapsto Ax \) est une application linéaire. \\
Par conséquent, \( \Image(x \mapsto Ax) \) est un espace-vectoriel. On peut donc regarder la projection orthogonale de \( b \) sur \( \Image(x \mapsto Ax) \), que nous noterons \( Ax^* \). On sait que \( |Ax - b|_2 \) est la distance usuelle entre \( Ax \) et \( b \), que nous noterons \( d(Ax, b) \). \\
Alors, par le thorème de Pythagore,
$$
\forall y \in \IR^N,~ |Ay - b|_2 = d(Ay, b) = \sqrt{d(Ax^*, b)^2 + d(Ax^*, Ay)^2} \geq d(Ax^*, b) = |Ax^* - b|_2
$$
On a donc bien que \( Ax^* \) est l'image du minimum de la fonction \eqref{eq:1} et comme \( Ax^* \in \Image(x \mapsto Ax) \), il existe \( x \in \IR^N \) tel que \( Ax = Ax^* \). Il reste à montrer l'unicité de ce minimum. \\
On sait par hypothèse que le rang de \( A \) est \( N \), càd \( \dim{(\Image(x \mapsto Ax))} = N \). \\
On sait également que \( \dim{(\operatorname{Dom}(x \mapsto Ax))} = \dim{(\IR^N)} = N \). Alors, par le théorème du rang la dimension du noyau de cette fonction est 0 càd la fonction est injective. Par conséquent, le minimum est bien unique, ce qui termine notre preuve.

\newpage

\noindent
Pour les questions suivantes, nous allons supposer que \( A \) et \( b \) varient dans le temps. \\
Soit donc \( A : \IR \to \IR^{M \times N} : t \mapsto A(t) \) et \( b : \IR \to \IR^M : t \mapsto b(t) \). Comme à chaque temps \( t \) on peut trouver l'unique solution \( x_t \) de l'équation \( (A(t))^T A(t) x_t = (A(t))^T b(t) \) \eqref{eq:3}, on définit la fonction \( x : \IR \to \IR^N : t \mapsto x(t) := x_t \). \\
Nous allons dans les exercices suivants nous intéresser à approximer la fonction \( \partial x \) en un temps \( t \) donné.

\section{Exercice 5}

\begin{algorithm}

\begin{algorithmic}[1]

\REQUIRE \( a,~ b \) des fonctions qui renvoient vers des matrices, \( da,~ db \) des fonctions qui renvoient vers les dérivées des matrices et \( t \) un réel qui représente le temps.
\ENSURE Retourne une approximation de la dérivée de la solution \( X \) au temps \( t \) de l'équation \( A^tAX = A^tb \).
\STATE \( at \leftarrow a(t) \)
\STATE \( dat \leftarrow da(t) \)
\STATE \( bt \leftarrow b(t) \)
\STATE \( x \leftarrow q1(at, b) \) \\
(La multiplication scalaire est réalisée par Lapack). \\
\STATE \( sol \leftarrow \) résultat de l'expression \( \partial_k A^t(t) \left( b(t) - A(t) * x(t) \right) + A^t(t) \left( \partial_k b(t) - \partial_k A(t) x(t) \right) \)
\RETURN le résultat de l'équation linéaire \( AX = b \) au temps t avec \( A = A^t * A \text{ et } b = sol \).
\caption{\textbf{Algorithme : }deriv}

\end{algorithmic}

\end{algorithm}
\noindent
On doit montrer que notre algorithme retourne bien \( \partial x(t) \). \\
Commençons par montrer que
$$
\forall M^*, N^*, K^* \in \IN^{\geq 1},~ \forall A : \IR \to \IR^{M^* \times K^*},~ \forall B : \IR \to \IR^{K^* \times N^*},~ \partial (AB) = (\partial A) B + A \partial(B)
$$
Soit \( M^*, N^*, K^* \in \IN^{\geq 1} \). Soit \( A : \IR \to \IR^{M^* \times K^*} \). Soit \( B : \IR \to \IR^{K^* \times N^*} \). On a alors
\begin{align*}
    \partial (AB)
    & = \partial \left(
        \begin{pmatrix}
        a_{1, 1} & \hdots & a_{1, K^*} \\
        \vdots & & \vdots \\
        a_{M^*, 1} & \hdots & a_{M^*, K^*}
        \end{pmatrix}
        \begin{pmatrix}
        b_{1, 1} & \hdots & b_{1, N^*} \\
        \vdots & & \vdots \\
        b_{K^*, 1} & \hdots & b_{K^*, N^*}
        \end{pmatrix} \right) \\
    & = \partial
        \begin{pmatrix}
        \displaystyle \sum_{k = 1}^{K^*} (a_{1, k} b_{k, 1}) & \hdots & \displaystyle \sum_{k = 1}^{K^*} (a_{1, k} b_{k, N^*}) \\
        \vdots & & \vdots \\
        \displaystyle \sum_{k = 1}^{K^*} (a_{M^*, k} b_{k, 1}) & \hdots & \displaystyle \sum_{k = 1}^{K^*} (a_{M^*, k} b_{k, N^*})
        \end{pmatrix} \\
    & = \begin{pmatrix}
        \displaystyle \sum_{k = 1}^{K^*} \partial (a_{1, k} b_{k, 1}) & \hdots & \displaystyle \sum_{k = 1}^{K^*} \partial (a_{1, k} b_{k, N^*}) \\
        \vdots & & \vdots \\
        \displaystyle \sum_{k = 1}^{K^*} \partial (a_{M^*, k} b_{k, 1}) & \hdots & \displaystyle \sum_{k = 1}^{K^*} \partial (a_{M^*, k} b_{k, N^*})
        \end{pmatrix} \\
    & = \begin{pmatrix}
        \displaystyle \sum_{k = 1}^{K^*} ((\partial a_{1, k}) b_{k, 1} + a_{1, k} \partial b_{k, 1} )) & \hdots & \displaystyle \sum_{k = 1}^{K^*} ((\partial a_{1, k}) b_{k, N^*} + a_{1, k} \partial b_{k, N^*} )) \\
        \vdots & & \vdots \\
        \displaystyle \sum_{k = 1}^{K^*} ((\partial a_{M^*, k}) b_{k, 1} + a_{M^*, k} \partial b_{k, 1} )) & \hdots & \displaystyle \sum_{k = 1}^{K^*} ((\partial a_{M^*, k}) b_{k, N^*} + a_{M^*, k} \partial b_{k, N^*} ))
        \end{pmatrix} \\
    & = \begin{pmatrix}
        \displaystyle \sum_{k = 1}^{K^*} ((\partial a_{1, k}) b_{k, 1}) & \hdots & \displaystyle \sum_{k = 1}^{K^*} ((\partial a_{1, k}) b_{k, N^*}) \\
        \vdots & & \vdots \\
        \displaystyle \sum_{k = 1}^{K^*} ((\partial a_{M^*, k}) b_{k, 1}) & \hdots & \displaystyle \sum_{k = 1}^{K^*} ((\partial a_{M^*, k}) b_{k, N^*})
        \end{pmatrix}
        +
        \begin{pmatrix}
        \displaystyle \sum_{k = 1}^{K^*} (a_{1, k} \partial b_{k, 1} )) & \hdots & \displaystyle \sum_{k = 1}^{K^*} (a_{1, k} \partial b_{k, N^*} )) \\
        \vdots & & \vdots \\
        \displaystyle \sum_{k = 1}^{K^*} (a_{M^*, k} \partial b_{k, 1} )) & \hdots & \displaystyle \sum_{k = 1}^{K^*} (a_{M^*, k} \partial b_{k, N^*} ))
        \end{pmatrix} \\
    & = (\partial A) B + A \partial(B)
\end{align*}
On a donc bien montré que la dérivée d'un produit se généralise aux fonctions allant des réels aux matrices finies et au produit matriciel. \\
Utilisons donc ce résultat afin de dériver chaque membre de l'équation \( (A(t))^T A(t) x(t) = (A(t))^T b(t) \). On a alors
$$
\partial ((A(t))^T A(t) x(t)) = (\partial (A(t))^T) A(t) x(t) + (A(t))^T \partial(A(t)) x(t) + (A(t))^T A(t) \partial x(t)
$$
et
$$
\partial ((A(t))^T b(t)) = \partial((A(t))^T) b(t) + (A(t))^T \partial b(t)
$$
Comme on sait que \( (A(t))^T A(t) x(t) = (A(t))^T b(t) \), on a
$$
(\partial (A(t))^T) A(t) x(t) + (A(t))^T \partial(A(t)) x(t) + (A(t))^T A(t) \partial x(t) = \partial((A(t))^T) b(t) + (A(t))^T \partial b(t)
$$
On connait \( x(t) \) car il s'agit de l'unique solution de l'équation \( A(t)^T A(t) x(t) = A^T(t) b(t) \) par \eqref{eq:3}. \\
Regardons donc l'équation
$$
(A(t))^T A(t) X = \partial((A(t))^T) (b(t) - A(t) x(t)) + (A(t))^T (\partial b(t) - \partial(A(t)) x(t))
$$
On sait que \( X = \partial x(t) \) en est une solution par l'égalité des dérivées marquées ci-dessus. \\
Il reste à montrer qu'il s'agit de l'unique solution.
$$
\textbf{Nous n'avons pas réussi à montrer l'unicité.}
$$
Comme il s'agit de l'unique solution, en la calculant via les méthodes de LAPACK on trouve bien \( \partial x(t) \), ce qui montre que notre algorithme est correct.

\end{document}